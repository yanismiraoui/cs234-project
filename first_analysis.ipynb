{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check:\n",
      "State 0, Action 0: [(1.0, 0, 0.0, False)]\n",
      "State 0, Action 1: [(1.0, 4, 0.0, False)]\n",
      "State 0, Action 2: [(1.0, 1, 0.0, False)]\n",
      "State 0, Action 3: [(1.0, 0, 0.0, False)]\n",
      "State 1, Action 0: [(1.0, 0, 0.0, False)]\n",
      "State 1, Action 1: [(1.0, 5, 0.0, True)]\n",
      "State 1, Action 2: [(1.0, 2, 0.0, False)]\n",
      "State 1, Action 3: [(1.0, 1, 0.0, False)]\n",
      "State 2, Action 0: [(1.0, 1, 0.0, False)]\n",
      "State 2, Action 1: [(1.0, 6, 0.0, False)]\n",
      "State 2, Action 2: [(1.0, 3, 0.0, False)]\n",
      "State 2, Action 3: [(1.0, 2, 0.0, False)]\n",
      "State 3, Action 0: [(1.0, 2, 0.0, False)]\n",
      "State 3, Action 1: [(1.0, 7, 0.0, True)]\n",
      "State 3, Action 2: [(1.0, 3, 0.0, False)]\n",
      "State 3, Action 3: [(1.0, 3, 0.0, False)]\n",
      "State 4, Action 0: [(1.0, 4, 0.0, False)]\n",
      "State 4, Action 1: [(1.0, 8, 0.0, False)]\n",
      "State 4, Action 2: [(1.0, 5, 0.0, True)]\n",
      "State 4, Action 3: [(1.0, 0, 0.0, False)]\n",
      "State 5, Action 0: [(1.0, 5, 0, True)]\n",
      "State 5, Action 1: [(1.0, 5, 0, True)]\n",
      "State 5, Action 2: [(1.0, 5, 0, True)]\n",
      "State 5, Action 3: [(1.0, 5, 0, True)]\n",
      "State 6, Action 0: [(1.0, 5, 0.0, True)]\n",
      "State 6, Action 1: [(1.0, 10, 0.0, False)]\n",
      "State 6, Action 2: [(1.0, 7, 0.0, True)]\n",
      "State 6, Action 3: [(1.0, 2, 0.0, False)]\n",
      "State 7, Action 0: [(1.0, 7, 0, True)]\n",
      "State 7, Action 1: [(1.0, 7, 0, True)]\n",
      "State 7, Action 2: [(1.0, 7, 0, True)]\n",
      "State 7, Action 3: [(1.0, 7, 0, True)]\n",
      "State 8, Action 0: [(1.0, 8, 0.0, False)]\n",
      "State 8, Action 1: [(1.0, 12, 0.0, True)]\n",
      "State 8, Action 2: [(1.0, 9, 0.0, False)]\n",
      "State 8, Action 3: [(1.0, 4, 0.0, False)]\n",
      "State 9, Action 0: [(1.0, 8, 0.0, False)]\n",
      "State 9, Action 1: [(1.0, 13, 0.0, False)]\n",
      "State 9, Action 2: [(1.0, 10, 0.0, False)]\n",
      "State 9, Action 3: [(1.0, 5, 0.0, True)]\n",
      "State 10, Action 0: [(1.0, 9, 0.0, False)]\n",
      "State 10, Action 1: [(1.0, 14, 0.0, False)]\n",
      "State 10, Action 2: [(1.0, 11, 0.0, True)]\n",
      "State 10, Action 3: [(1.0, 6, 0.0, False)]\n",
      "State 11, Action 0: [(1.0, 11, 0, True)]\n",
      "State 11, Action 1: [(1.0, 11, 0, True)]\n",
      "State 11, Action 2: [(1.0, 11, 0, True)]\n",
      "State 11, Action 3: [(1.0, 11, 0, True)]\n",
      "State 12, Action 0: [(1.0, 12, 0, True)]\n",
      "State 12, Action 1: [(1.0, 12, 0, True)]\n",
      "State 12, Action 2: [(1.0, 12, 0, True)]\n",
      "State 12, Action 3: [(1.0, 12, 0, True)]\n",
      "State 13, Action 0: [(1.0, 12, 0.0, True)]\n",
      "State 13, Action 1: [(1.0, 13, 0.0, False)]\n",
      "State 13, Action 2: [(1.0, 14, 0.0, False)]\n",
      "State 13, Action 3: [(1.0, 9, 0.0, False)]\n",
      "State 14, Action 0: [(1.0, 13, 0.0, False)]\n",
      "State 14, Action 1: [(1.0, 14, 0.0, False)]\n",
      "State 14, Action 2: [(1.0, 15, 1.0, True)]\n",
      "State 14, Action 3: [(1.0, 10, 0.0, False)]\n",
      "State 15, Action 0: [(1.0, 15, 0, True)]\n",
      "State 15, Action 1: [(1.0, 15, 0, True)]\n",
      "State 15, Action 2: [(1.0, 15, 0, True)]\n",
      "State 15, Action 3: [(1.0, 15, 0, True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanis/miniforge3/envs/cs234_hw3/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def one_hot(n, i):\n",
    "    vec = np.zeros(n)\n",
    "    vec[i] = 1\n",
    "    return vec\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, nA, epsilon=0.1):\n",
    "    policy = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    policy[best_action] += (1.0 - epsilon)\n",
    "    return policy\n",
    "\n",
    "def evaluate_policy(env, Q, s0, gamma=0.99, num_episodes=100):\n",
    "    total_rewards = 0\n",
    "    for _ in tqdm(range(num_episodes)):\n",
    "        state = s0\n",
    "        done = False\n",
    "        episode_rewards = 0\n",
    "        discount_factor = 1\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode_rewards += discount_factor * reward\n",
    "            discount_factor *= gamma\n",
    "            state = next_state\n",
    "\n",
    "        total_rewards += episode_rewards\n",
    "\n",
    "    avg_reward = total_rewards / num_episodes\n",
    "    print(f\"Average reward for policy starting at state {s0}: {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "def find_optimal_policy(env, gamma=0.99):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS)\n",
    "    Q = np.zeros((nS, nA))\n",
    "\n",
    "    for i in range(1000):  # A large number to ensure convergence\n",
    "        for s in range(nS):\n",
    "            Q[s] = [sum([p * (r + gamma * V[s_])\n",
    "                         for p, s_, r, _ in env.P[s][a]]) for a in range(nA)]\n",
    "        V = np.max(Q, axis=1)\n",
    "\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    return policy, Q, V\n",
    "\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "env.reset()\n",
    "print(\"Environment Check:\")\n",
    "for state in range(env.observation_space.n):\n",
    "    for action in range(env.action_space.n):\n",
    "        print(f\"State {state}, Action {action}: {env.P[state][action]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_ascent(env, s0, gamma=0.99, alpha=0.5, epsilon=0.01, max_iterations=10000):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    iteration_count = 0\n",
    "\n",
    "    optimal_policy, optimal_Q, optimal_V = find_optimal_policy(env)\n",
    "    V_star_s0 = optimal_V[s0]\n",
    "\n",
    "    while True:\n",
    "        iteration_count += 1\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            policy = epsilon_greedy_policy(Q, state, nA)\n",
    "            action = np.random.choice(np.arange(nA), p=policy)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        V_pi_s0 = evaluate_policy(env, Q, s0, gamma)\n",
    "        print(f\"Iteration {iteration_count}, V_pi_s0: {V_pi_s0:.4f}, V_star_s0: {V_star_s0:.4f}, TD Error: {td_error:.4f}\")\n",
    "        if V_star_s0 - V_pi_s0 <= epsilon or iteration_count >= max_iterations:\n",
    "            break\n",
    "\n",
    "    return Q, iteration_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_policy_gradient(env, s0, gamma=0.99, alpha=0.1, epsilon=0.01, max_iterations=10000):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    theta = np.zeros((nS, nA))\n",
    "    iteration_count = 0\n",
    "\n",
    "    optimal_policy, optimal_Q, optimal_V = find_optimal_policy(env)\n",
    "    V_star_s0 = optimal_V[s0]\n",
    "\n",
    "    while True:\n",
    "        iteration_count += 1\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            probs = softmax(theta[state])\n",
    "            action = np.random.choice(np.arange(nA), p=probs)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            best_next_action = np.argmax(theta[next_state])\n",
    "            td_target = reward + gamma * theta[next_state][best_next_action]\n",
    "            td_error = td_target - theta[state][action]\n",
    "\n",
    "            gradient = one_hot(nA, action) - probs\n",
    "            theta[state] += alpha * td_error * gradient\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        Q = {s: softmax(theta[s]) for s in range(nS)}\n",
    "        V_pi_s0 = evaluate_policy(env, Q, s0, gamma)\n",
    "        print(f\"Iteration {iteration_count}, V_pi_s0: {V_pi_s0:.4f}, V_star_s0: {V_star_s0:.4f}\")\n",
    "        if V_star_s0 - V_pi_s0 <= epsilon or iteration_count >= max_iterations:\n",
    "            break\n",
    "\n",
    "    return theta, iteration_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_policy_gradient(env, s0, gamma=0.99, alpha=0.1, epsilon=0.01, max_iterations=10000):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    theta = np.zeros((nS, nA))\n",
    "    iteration_count = 0\n",
    "\n",
    "    optimal_policy, optimal_Q, optimal_V = find_optimal_policy(env)\n",
    "    V_star_s0 = optimal_V[s0]\n",
    "\n",
    "    while True:\n",
    "        iteration_count += 1\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        I = 1\n",
    "\n",
    "        while not done:\n",
    "            probs = softmax(theta[state])\n",
    "            action = np.random.choice(np.arange(nA), p=probs)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            best_next_action = np.argmax(theta[next_state])\n",
    "            td_target = reward + gamma * np.max(theta[next_state])\n",
    "            td_error = td_target - theta[state][action]\n",
    "\n",
    "            gradient = one_hot(nA, action) - probs\n",
    "            theta[state] += alpha * I * td_error * gradient\n",
    "            I *= gamma\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        Q = {s: softmax(theta[s]) for s in range(nS)}\n",
    "        V_pi_s0 = evaluate_policy(env, Q, s0, gamma)\n",
    "        print(f\"Iteration {iteration_count}, V_pi_s0: {V_pi_s0:.4f}, V_star_s0: {V_star_s0:.4f}\")\n",
    "        if V_star_s0 - V_pi_s0 <= epsilon or iteration_count >= max_iterations:\n",
    "            break\n",
    "\n",
    "    return theta, iteration_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 3, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 4, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 5, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 6, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 7, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 8, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 9, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 10, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 11, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 12, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 13, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 14, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 15, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 16, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 17, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 18, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 19, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 20, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 21, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 22, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 23, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 24, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 25, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 26, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 27, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 28, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 29, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 30, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 31, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 32, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 33, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 34, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 35, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 36, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 37, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 38, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 39, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 40, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 41, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 42, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 43, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 44, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 45, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 46, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 47, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 48, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 49, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 50, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 51, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 52, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 53, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 54, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 55, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 56, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 57, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 58, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 59, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 60, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 61, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 62, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 63, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 64, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 65, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 66, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 67, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 68, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 69, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 70, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 71, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 72, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 73, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 74, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 75, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 76, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 77, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 78, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 79, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 80, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 81, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 82, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 83, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 84, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 85, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 86, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 87, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 88, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 89, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 90, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 91, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 92, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 93, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 94, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 95, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 96, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 97, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 98, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 99, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 100, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 101, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 102, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 103, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 104, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 105, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 106, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 107, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 108, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 109, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 110, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 111, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 112, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 113, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 114, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 115, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 116, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 117, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 118, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 119, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 120, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 121, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 122, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 123, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 124, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 125, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 126, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 127, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 128, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 129, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 130, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 131, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 132, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 133, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 134, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 135, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 136, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 137, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 138, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 139, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 140, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 141, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 142, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 143, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 144, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 145, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 146, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 147, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 148, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 149, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 150, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 151, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 152, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 153, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 154, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 155, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 156, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 157, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 158, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 159, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 160, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 161, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 162, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 163, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 164, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 165, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 166, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 167, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 168, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 169, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 170, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 171, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 172, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 173, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 174, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 175, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 176, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 177, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 178, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 179, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 180, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 181, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 182, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 183, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 184, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 185, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 186, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 187, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 188, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 189, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 190, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 191, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 192, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 193, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 194, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 195, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 196, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 197, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 198, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 199, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 200, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 201, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 202, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 203, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 204, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 205, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 206, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 207, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 208, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 209, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 210, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 211, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 212, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 213, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 214, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 215, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 216, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 217, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 218, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 219, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 220, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 221, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 222, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 223, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 224, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 225, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 226, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 227, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 228, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 229, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 230, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 231, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 232, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 233, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 234, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 235, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 236, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 237, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 238, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 239, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 240, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 241, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 242, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 243, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 244, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 245, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 246, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 247, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 248, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 249, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 250, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 251, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 252, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 253, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 254, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 255, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 256, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 257, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 258, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 259, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 260, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 261, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 262, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 263, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 264, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 265, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 266, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 267, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 268, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 269, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 270, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 271, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 272, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 273, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 274, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 275, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 276, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 277, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 278, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 279, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 280, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 281, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 282, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 283, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 284, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 285, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 286, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 287, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 288, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 289, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 290, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 291, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 292, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 293, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 294, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 295, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 296, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 297, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 298, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 299, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 300, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 301, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 302, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 303, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 304, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 305, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 306, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 307, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 308, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 309, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 310, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 311, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 312, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 313, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 314, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 315, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 316, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 317, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 318, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 319, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 320, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 321, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 322, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 323, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 324, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 325, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 326, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 327, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 328, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 329, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 330, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 331, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 332, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 333, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 334, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 335, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 336, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 337, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 338, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 339, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 340, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 341, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 342, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 343, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 344, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 345, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 346, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 347, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 348, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 349, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 350, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 351, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 352, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 353, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 354, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 355, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 356, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 357, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 358, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 359, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 360, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 361, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 362, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 363, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 364, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 365, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 366, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 367, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 368, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 369, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 370, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 371, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 372, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 373, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 374, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 375, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 376, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 377, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 378, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 379, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 380, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 381, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 382, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 383, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 384, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 385, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 386, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 387, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 388, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 389, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 390, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 391, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 392, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 393, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 394, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 395, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 396, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 397, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 398, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 399, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 400, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 401, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 402, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 403, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 404, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 405, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 406, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 407, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 408, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 409, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 410, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 411, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 412, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 413, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 414, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 415, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 416, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 417, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 418, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 419, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 420, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 421, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 422, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 423, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 424, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 425, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 426, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 427, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 428, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 429, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 430, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 431, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 432, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 433, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 434, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 435, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 436, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 437, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 438, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 439, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 440, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 441, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 442, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 443, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 444, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 445, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 446, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 447, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 448, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 449, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 450, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 451, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 452, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 453, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 454, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 455, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 456, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 457, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 458, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 459, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 460, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 461, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 462, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 463, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 464, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 465, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 466, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 467, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 468, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 469, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 470, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 471, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 472, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 473, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 474, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 475, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 476, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 477, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 478, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 479, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 480, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 481, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 482, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 483, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 484, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 485, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 486, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 487, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 488, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 489, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 490, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 491, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 492, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 493, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 494, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 495, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 496, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 497, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 498, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 499, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 500, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 501, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 502, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 503, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 504, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 505, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 506, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 507, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 508, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 509, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 510, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 511, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 512, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 513, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 514, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 515, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 516, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 517, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 518, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 519, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 520, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 521, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 522, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 523, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 524, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 525, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 526, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 527, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 528, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 529, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 530, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 531, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 532, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 533, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 534, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 535, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 536, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 537, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 538, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 539, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 540, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 541, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 542, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 543, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 544, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 545, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 546, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 547, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 548, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 549, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 550, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 551, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 552, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 553, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 554, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 555, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 556, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 557, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 558, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 559, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 560, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 561, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 562, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 563, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 564, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 565, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 566, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 567, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 568, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 569, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 570, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 571, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 572, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 573, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 574, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 575, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 576, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 577, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 578, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 579, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 580, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 581, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 582, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 583, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 584, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 585, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 586, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 587, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 588, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 589, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 590, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 591, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 592, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 593, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 594, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 595, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 596, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 597, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 598, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 599, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 600, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 601, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 602, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 603, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 604, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 605, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 606, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 607, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 608, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 609, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 610, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 611, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 612, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 613, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 614, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 615, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 616, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 617, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 618, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 619, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 620, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 621, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 622, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 623, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 624, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 625, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 626, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 627, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 628, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 629, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 630, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 631, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 632, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 633, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 634, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 635, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 636, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 637, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 638, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 639, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 640, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 641, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 642, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 643, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 644, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 645, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 646, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 647, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 648, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 649, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 650, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 651, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 652, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 653, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 654, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 655, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 656, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 657, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 658, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 659, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 660, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 661, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 662, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 663, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 664, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 665, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 666, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 667, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 668, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 669, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 670, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 671, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 672, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 673, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 674, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 675, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 676, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 677, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 678, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 679, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 680, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 681, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 682, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 683, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 684, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 685, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 686, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 687, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 688, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 689, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 690, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 691, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 692, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 693, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 694, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 695, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 696, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 697, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 698, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 699, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 700, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 701, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 702, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 703, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 704, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 705, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 706, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 707, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 708, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 709, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 710, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 711, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 712, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 713, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 714, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 715, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 716, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 717, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 718, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 719, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 720, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 721, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 722, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 723, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 724, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 725, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 726, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 727, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 728, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 729, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 730, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 731, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 732, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 733, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 734, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 735, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 736, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 737, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 738, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 739, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 740, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 741, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 742, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 743, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 744, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 745, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 746, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 747, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 748, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 749, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 750, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 751, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 752, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 753, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 754, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 755, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 756, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 757, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 758, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 759, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 760, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 761, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 762, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 763, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 764, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 765, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 766, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 767, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 768, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 769, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 770, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 771, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 772, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 773, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 774, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 775, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 776, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 777, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 778, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 779, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 780, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 781, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 782, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 783, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 784, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 785, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 786, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 787, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 788, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 789, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 790, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 791, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 792, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 793, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 794, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 795, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 796, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 797, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 798, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 799, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 800, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 801, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 802, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 803, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 804, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 805, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 806, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 807, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 808, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 809, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 810, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 811, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 812, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 813, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 814, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 815, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 816, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 817, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 818, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 819, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 820, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 821, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 822, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 823, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 824, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 825, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 826, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 827, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 828, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 829, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 830, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 831, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 832, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 833, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 834, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 835, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 836, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 837, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 838, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 839, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 840, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 841, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 842, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 843, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 844, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 845, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 846, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 847, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 848, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 849, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 850, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 851, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 852, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 853, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 854, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 855, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 856, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 857, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 858, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 859, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 860, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 861, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 862, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 863, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 864, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 865, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 866, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 867, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 868, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 869, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 870, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 871, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 872, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 873, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 874, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 875, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 876, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 877, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 878, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 879, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 880, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 881, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 882, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 883, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 884, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 885, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 886, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 887, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 888, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 889, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 890, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 891, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 892, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 893, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 894, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 895, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 896, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 897, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 898, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 899, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 900, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 901, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 902, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 903, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 904, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 905, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 906, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 907, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 908, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 909, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 910, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 911, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 912, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 913, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 914, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 915, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 916, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 917, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 918, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 919, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 920, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 921, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 922, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 923, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 924, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 925, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 926, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 927, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 928, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 929, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 930, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 931, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 932, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 933, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 934, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 935, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 936, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 937, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 938, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 939, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 940, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 941, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 942, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 943, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 944, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 945, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 946, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 947, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 948, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 949, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 950, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 951, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 952, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 953, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 954, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 955, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 956, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 957, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 958, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 959, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 960, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 961, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 962, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 963, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 964, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 965, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 966, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 967, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 968, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 969, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 970, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 971, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 972, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 973, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 974, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 975, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 976, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 977, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 978, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 979, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 980, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 981, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 982, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 983, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 984, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 985, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 986, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 987, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 988, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 989, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 990, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 991, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 992, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 993, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 994, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 995, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 996, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 997, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 998, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 999, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1000, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1001, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1002, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1003, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1004, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1005, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1006, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1007, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1008, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1009, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1010, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1011, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1012, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1013, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1014, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1015, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1016, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1017, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1018, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1019, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1020, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1021, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1022, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1023, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1024, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1025, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1026, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1027, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1028, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1029, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1030, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1031, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1032, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1033, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1034, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1035, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1036, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1037, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1038, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1039, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1040, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1041, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1042, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1043, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1044, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1045, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1046, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1047, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1048, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1049, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1050, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1051, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1052, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1053, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1054, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1055, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1056, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1057, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1058, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1059, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1060, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1061, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1062, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1063, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1064, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1065, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1066, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1067, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1068, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1069, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1070, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1071, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1072, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1073, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1074, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1075, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1076, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1077, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1078, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1079, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1080, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1081, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1082, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1083, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1084, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1085, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1086, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1087, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1088, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1089, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1090, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1091, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1092, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1093, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1094, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1095, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1096, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1097, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1098, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1099, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1100, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1101, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1102, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1103, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1104, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1105, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1106, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1107, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1108, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1109, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1110, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1111, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1112, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1113, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1114, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1115, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1116, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1117, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1118, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1119, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1120, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1121, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1122, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1123, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1124, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1125, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1126, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1127, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1128, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1129, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1130, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1131, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1132, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1133, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1134, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1135, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1136, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1137, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1138, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1139, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1140, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1141, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1142, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1143, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1144, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1145, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1146, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1147, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1148, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1149, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1150, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1151, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1152, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1153, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1154, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1155, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1156, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1157, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1158, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1159, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1160, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1161, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1162, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1163, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1164, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1165, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1166, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1167, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1168, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1169, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1170, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1171, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1172, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1173, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1174, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1175, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1176, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1177, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1178, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1179, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1180, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1181, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1182, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1183, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1184, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1185, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1186, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1187, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1188, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1189, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1190, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1191, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1192, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1193, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1194, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1195, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1196, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1197, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1198, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1199, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1200, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1201, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1202, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1203, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1204, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1205, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1206, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1207, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1208, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1209, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1210, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1211, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1212, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1213, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1214, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1215, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1216, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1217, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1218, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1219, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1220, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1221, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1222, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1223, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1224, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1225, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1226, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1227, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1228, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1229, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1230, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1231, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1232, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1233, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1234, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1235, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1236, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1237, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1238, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1239, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1240, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1241, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1242, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1243, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1244, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1245, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1246, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1247, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1248, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1249, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1250, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1251, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1252, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1253, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1254, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1255, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1256, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1257, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1258, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1259, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1260, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1261, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1262, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1263, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1264, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1265, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1266, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1267, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1268, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1269, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1270, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1271, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1272, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1273, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1274, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1275, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1276, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1277, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1278, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1279, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1280, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1281, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1282, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1283, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1284, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1285, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1286, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1287, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1288, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1289, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1290, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1291, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1292, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1293, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1294, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1295, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1296, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1297, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1298, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1299, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1300, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1301, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1302, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1303, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1304, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1305, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1306, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1307, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1308, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1309, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1310, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1311, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1312, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1313, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1314, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1315, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1316, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1317, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1318, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1319, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1320, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1321, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1322, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1323, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1324, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1325, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1326, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1327, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1328, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1329, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1330, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1331, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1332, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1333, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1334, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1335, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1336, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1337, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1338, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1339, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1340, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1341, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1342, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1343, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1344, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1345, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1346, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1347, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1348, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1349, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1350, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1351, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1352, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1353, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1354, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1355, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1356, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1357, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1358, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1359, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1360, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1361, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1362, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1363, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1364, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1365, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1366, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1367, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1368, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1369, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1370, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1371, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1372, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1373, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1374, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1375, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1376, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1377, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1378, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1379, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1380, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1381, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1382, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1383, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1384, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1385, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1386, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1387, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1388, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1389, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1390, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1391, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1392, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1393, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1394, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1395, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1396, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1397, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1398, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1399, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1400, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1401, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1402, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1403, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1404, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1405, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1406, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1407, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1408, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1409, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1410, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1411, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1412, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1413, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1414, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1415, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1416, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1417, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1418, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1419, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1420, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1421, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1422, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1423, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1424, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1425, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1426, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1427, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1428, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1429, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1430, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1431, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1432, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1433, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1434, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1435, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1436, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1437, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1438, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1439, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1440, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1441, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1442, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1443, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1444, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1445, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1446, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1447, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1448, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1449, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1450, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1451, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1452, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1453, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1454, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1455, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1456, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1457, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1458, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1459, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1460, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1461, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1462, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1463, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1464, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1465, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1466, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1467, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1468, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1469, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1470, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1471, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1472, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1473, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1474, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1475, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1476, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1477, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1478, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1479, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1480, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1481, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1482, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1483, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1484, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1485, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1486, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1487, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1488, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1489, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1490, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1491, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1492, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1493, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1494, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1495, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1496, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1497, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1498, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1499, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1500, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1501, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1502, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1503, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1504, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1505, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1506, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1507, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1508, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1509, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1510, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1511, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1512, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1513, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1514, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1515, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1516, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1517, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1518, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1519, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1520, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1521, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1522, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1523, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1524, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1525, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1526, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1527, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1528, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1529, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1530, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1531, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1532, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1533, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1534, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1535, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1536, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1537, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1538, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1539, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1540, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1541, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1542, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1543, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1544, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1545, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1546, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1547, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1548, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1549, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1550, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1551, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1552, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1553, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1554, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1555, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1556, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1557, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1558, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1559, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1560, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1561, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1562, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1563, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1564, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1565, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1566, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1567, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1568, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1569, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1570, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1571, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1572, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1573, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1574, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1575, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1576, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1577, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1578, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1579, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1580, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1581, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1582, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1583, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1584, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1585, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1586, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1587, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1588, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1589, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1590, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1591, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1592, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1593, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1594, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1595, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1596, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1597, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1598, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1599, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1600, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1601, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1602, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1603, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1604, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1605, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1606, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1607, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1608, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1609, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1610, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1611, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1612, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1613, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1614, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1615, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1616, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1617, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1618, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1619, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1620, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1621, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1622, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1623, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1624, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1625, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1626, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1627, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1628, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1629, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1630, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1631, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1632, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1633, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1634, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1635, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1636, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1637, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1638, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1639, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1640, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1641, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1642, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1643, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1644, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1645, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1646, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1647, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1648, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1649, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1650, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1651, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1652, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1653, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1654, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1655, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1656, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1657, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1658, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1659, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1660, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1661, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1662, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1663, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1664, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1665, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1666, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1667, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1668, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1669, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1670, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1671, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1672, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1673, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1674, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1675, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1676, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1677, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1678, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1679, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1680, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1681, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1682, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1683, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1684, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1685, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1686, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1687, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1688, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1689, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1690, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1691, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1692, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1693, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1694, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1695, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1696, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1697, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1698, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1699, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1700, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1701, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1702, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1703, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1704, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1705, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1706, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1707, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1708, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1709, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1710, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1711, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1712, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1713, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1714, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1715, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1716, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1717, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1718, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1719, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1720, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1721, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1722, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1723, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1724, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1725, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1726, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1727, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1728, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1729, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1730, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1731, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1732, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1733, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1734, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1735, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1736, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1737, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1738, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1739, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1740, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1741, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1742, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1743, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1744, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1745, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1746, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1747, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1748, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1749, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1750, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1751, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1752, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1753, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1754, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1755, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1756, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1757, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1758, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1759, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1760, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1761, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1762, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1763, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1764, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1765, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1766, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1767, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1768, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1769, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1770, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1771, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1772, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1773, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1774, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1775, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1776, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1777, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1778, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1779, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1780, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1781, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1782, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1783, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1784, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1785, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1786, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1787, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1788, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1789, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1790, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1791, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1792, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1793, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1794, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1795, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1796, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1797, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1798, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1799, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1800, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1801, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1802, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1803, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1804, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1805, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1806, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1807, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1808, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1809, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1810, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1811, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1812, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1813, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1814, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1815, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1816, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1817, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1818, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1819, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1820, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1821, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1822, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1823, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1824, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1825, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1826, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1827, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1828, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1829, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1830, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1831, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1832, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1833, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1834, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1835, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1836, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1837, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1838, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1839, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1840, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1841, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1842, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1843, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1844, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1845, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1846, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1847, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1848, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1849, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1850, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1851, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1852, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1853, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1854, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1855, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1856, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1857, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1858, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1859, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1860, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1861, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1862, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1863, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1864, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1865, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1866, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1867, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1868, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1869, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1870, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1871, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1872, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1873, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1874, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1875, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1876, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1877, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1878, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1879, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1880, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1881, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1882, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1883, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1884, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1885, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1886, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1887, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1888, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1889, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1890, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1891, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1892, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1893, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1894, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1895, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1896, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1897, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1898, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1899, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1900, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1901, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1902, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1903, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1904, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1905, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1906, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1907, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1908, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1909, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1910, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1911, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1912, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1913, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1914, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1915, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1916, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1917, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1918, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1919, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1920, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1921, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1922, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1923, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1924, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1925, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1926, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1927, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1928, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1929, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1930, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1931, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1932, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1933, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1934, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1935, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1936, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1937, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1938, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1939, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1940, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1941, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1942, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1943, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1944, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1945, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1946, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1947, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1948, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1949, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1950, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1951, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1952, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1953, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1954, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1955, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1956, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1957, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1958, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1959, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1960, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1961, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1962, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1963, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1964, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1965, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1966, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1967, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1968, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1969, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1970, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1971, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1972, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1973, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1974, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1975, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1976, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1977, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1978, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1979, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1980, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1981, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1982, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1983, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1984, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1985, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1986, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1987, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1988, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1989, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1990, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1991, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1992, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1993, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1994, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1995, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1996, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1997, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1998, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 1999, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2000, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2001, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2002, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2003, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2004, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2005, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2006, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2007, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2008, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2009, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2010, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2011, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2012, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2013, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2014, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2015, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2016, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2017, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2018, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2019, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2020, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2021, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2022, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2023, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2024, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2025, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2026, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2027, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2028, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2029, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2030, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2031, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2032, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2033, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2034, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2035, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2036, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2037, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2038, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2039, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2040, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2041, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2042, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2043, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2044, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2045, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2046, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2047, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2048, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2049, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2050, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2051, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2052, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2053, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2054, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2055, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2056, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2057, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2058, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2059, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2060, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2061, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2062, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2063, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2064, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2065, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2066, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2067, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2068, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2069, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2070, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2071, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2072, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2073, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2074, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2075, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2076, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2077, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2078, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2079, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2080, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2081, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2082, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2083, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2084, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2085, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2086, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2087, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2088, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2089, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2090, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2091, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2092, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2093, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2094, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2095, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2096, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2097, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2098, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2099, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2100, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2101, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2102, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2103, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2104, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2105, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2106, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2107, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2108, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2109, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2110, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2111, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2112, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2113, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2114, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2115, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2116, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2117, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2118, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2119, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2120, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2121, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2122, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2123, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2124, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2125, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2126, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2127, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2128, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2129, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2130, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2131, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2132, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2133, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2134, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2135, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2136, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2137, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2138, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2139, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2140, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2141, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2142, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2143, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2144, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2145, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2146, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2147, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2148, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2149, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2150, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2151, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2152, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2153, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2154, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2155, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2156, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2157, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2158, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2159, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2160, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2161, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2162, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2163, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2164, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2165, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2166, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2167, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2168, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2169, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2170, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2171, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2172, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2173, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2174, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2175, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2176, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2177, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2178, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2179, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2180, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2181, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2182, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2183, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2184, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2185, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2186, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2187, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2188, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2189, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2190, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2191, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2192, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2193, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2194, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2195, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2196, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2197, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2198, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2199, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2200, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2201, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2202, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2203, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2204, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2205, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2206, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2207, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2208, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2209, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2210, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2211, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2212, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2213, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2214, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2215, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2216, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2217, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2218, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2219, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2220, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2221, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2222, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2223, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2224, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2225, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2226, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2227, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2228, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2229, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2230, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2231, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2232, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2233, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2234, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2235, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2236, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2237, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2238, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2239, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2240, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2241, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2242, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2243, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2244, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2245, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2246, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2247, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2248, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2249, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2250, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2251, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2252, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2253, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2254, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2255, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2256, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2257, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2258, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2259, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2260, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2261, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2262, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2263, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2264, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2265, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2266, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2267, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2268, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2269, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2270, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2271, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2272, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2273, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2274, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2275, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2276, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2277, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2278, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2279, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2280, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2281, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2282, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2283, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2284, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2285, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2286, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2287, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2288, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2289, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2290, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2291, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2292, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2293, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2294, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2295, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2296, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2297, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2298, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2299, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2300, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2301, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2302, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2303, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2304, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2305, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2306, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2307, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2308, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2309, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2310, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2311, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2312, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2313, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2314, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2315, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2316, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2317, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2318, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2319, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2320, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2321, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2322, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2323, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2324, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2325, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2326, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2327, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2328, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2329, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2330, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2331, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2332, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2333, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2334, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2335, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2336, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2337, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2338, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2339, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2340, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2341, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2342, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2343, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2344, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2345, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2346, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2347, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2348, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2349, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2350, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2351, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2352, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2353, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2354, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2355, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2356, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2357, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2358, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2359, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2360, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2361, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2362, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2363, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2364, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2365, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2366, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2367, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2368, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2369, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2370, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2371, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2372, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2373, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2374, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2375, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2376, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2377, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2378, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2379, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2380, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2381, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2382, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2383, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2384, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2385, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2386, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2387, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2388, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2389, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2390, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2391, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2392, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2393, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2394, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2395, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2396, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2397, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2398, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2399, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2400, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2401, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2402, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2403, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2404, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2405, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2406, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2407, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2408, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2409, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2410, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2411, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2412, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2413, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2414, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2415, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2416, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2417, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2418, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2419, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2420, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2421, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2422, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2423, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2424, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2425, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2426, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2427, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2428, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2429, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2430, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2431, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2432, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2433, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2434, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n",
      "Average reward for policy starting at state 0: 0.0\n",
      "Iteration 2435, V_pi_s0: 0.0000, V_star_s0: 0.9510, TD Error: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m s0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Initial state\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Projected Gradient Ascent\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m Q_pga, iterations_pga \u001b[38;5;241m=\u001b[39m \u001b[43mprojected_gradient_ascent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m reward_pga \u001b[38;5;241m=\u001b[39m evaluate_policy(env, Q_pga, s0)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Softmax Policy Gradient\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[68], line 17\u001b[0m, in \u001b[0;36mprojected_gradient_ascent\u001b[0;34m(env, s0, gamma, alpha, epsilon, max_iterations)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     16\u001b[0m     policy \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(Q, state, nA)\n\u001b[0;32m---> 17\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     20\u001b[0m     best_next_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[next_state])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "s0 = 0 \n",
    "\n",
    "Q_pga, iterations_pga = projected_gradient_ascent(env, s0)\n",
    "reward_pga = evaluate_policy(env, Q_pga, s0)\n",
    "\n",
    "theta_spg, iterations_spg = softmax_policy_gradient(env, s0)\n",
    "Q_spg = {s: softmax(theta_spg[s]) for s in range(env.observation_space.n)}\n",
    "reward_spg = evaluate_policy(env, Q_spg, s0)\n",
    "\n",
    "theta_npg, iterations_npg = natural_policy_gradient(env, s0)\n",
    "Q_npg = {s: softmax(theta_npg[s]) for s in range(env.observation_space.n)}\n",
    "reward_npg = evaluate_policy(env, Q_npg, s0)\n",
    "\n",
    "print(f\"Projected Gradient Ascent: Reward = {reward_pga}, Iterations = {iterations_pga}\")\n",
    "print(f\"Softmax Policy Gradient: Reward = {reward_spg}, Iterations = {iterations_spg}\")\n",
    "print(f\"Natural Policy Gradient: Reward = {reward_npg}, Iterations = {iterations_npg}\")\n",
    "\n",
    "D_inf = 1\n",
    "S = env.observation_space.n\n",
    "A = env.action_space.n\n",
    "gamma = 0.99\n",
    "epsilon = 0.01\n",
    "\n",
    "def theoretical_iterations_pga(D_inf, S, A, gamma, epsilon):\n",
    "    return (D_inf**2 * S * A) / ((1 - gamma)**6 * epsilon**2)\n",
    "\n",
    "def theoretical_iterations_spg(D_inf, S, A, gamma, epsilon):\n",
    "    return (D_inf**2 * S**2 * A**2) / ((1 - gamma)**6 * epsilon**2)\n",
    "\n",
    "def theoretical_iterations_npg(gamma, epsilon):\n",
    "    return 2 / ((1 - gamma)**2 * epsilon)\n",
    "\n",
    "theoretical_pga = theoretical_iterations_pga(D_inf, S, A, gamma, epsilon)\n",
    "theoretical_spg = theoretical_iterations_spg(D_inf, S, A, gamma, epsilon)\n",
    "theoretical_npg = theoretical_iterations_npg(gamma, epsilon)\n",
    "\n",
    "print(f\"Theoretical Projected Gradient Ascent: {theoretical_pga}\")\n",
    "print(f\"Theoretical Softmax Policy Gradient: {theoretical_spg}\")\n",
    "print(f\"Theoretical Natural Policy Gradient: {theoretical_npg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050975bbc6b7486b9bd8a672a24ee005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7723073b6c474b2381c718b663e64e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2567b18ad83a4ea8a310654e80fcb8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4ee79d40084461951870c2bb4311f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14416344cbdd4785bb0d530c6d577ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcfb6ecead84614bbb8ed4af4764a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16 4 0.2 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d551c17caf4b4f2faf89ccd696770fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673697462fe14b6e8ae7519dd66db835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b006d99c714040fb9fd69a7a31271cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b37a4ff3be48ca90504f564dbb0449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df61198d15fb400ebed1cdb5b0b9f5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d47f62eb2b447498950ef364fc3ae72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16 4 0.5 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75747525ff674321aa7aa29d93143435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06a115805a64320a78f76663bc4841b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ea4bd21e3a48f1813ac2c9b861e491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e243fd4948114622819ca5778e6f1e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d980b2fed934e7b80730427143bda90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce27f3d071f4c6194548dcd2e52ee75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16 4 0.7 0.1\n",
      "Gamma: 0.2, Epsilon: 0.1\n",
      "Projected Gradient Ascent: Empirical = 14793, Theoretical = 24414.06249999999, Reward = 0.0\n",
      "Softmax Policy Gradient: Empirical = 7352, Theoretical = 1562499.9999999993, Reward = 0.08\n",
      "Natural Policy Gradient: Empirical = 7417, Theoretical = 31.249999999999993, Reward = 0.08\n",
      "\n",
      "Gamma: 0.5, Epsilon: 0.1\n",
      "Projected Gradient Ascent: Empirical = 14543, Theoretical = 409599.99999999994, Reward = 0.0\n",
      "Softmax Policy Gradient: Empirical = 7736, Theoretical = 26214399.999999996, Reward = 0.06\n",
      "Natural Policy Gradient: Empirical = 7583, Theoretical = 80.0, Reward = 0.05\n",
      "\n",
      "Gamma: 0.7, Epsilon: 0.1\n",
      "Projected Gradient Ascent: Empirical = 14703, Theoretical = 8779149.51989025, Reward = 0.0\n",
      "Softmax Policy Gradient: Empirical = 7533, Theoretical = 561865569.272976, Reward = 0.05\n",
      "Natural Policy Gradient: Empirical = 7650, Theoretical = 222.22222222222214, Reward = 0.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "gamma_values = [0.2, 0.5, 0.7]\n",
    "epsilon_values = [0.1]\n",
    "\n",
    "results = []\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    for epsilon in epsilon_values:\n",
    "        Q_pga, iterations_pga = projected_gradient_ascent(env, gamma=gamma, epsilon=epsilon)\n",
    "        reward_pga = evaluate_policy(env, Q_pga)\n",
    "\n",
    "        theta_spg, iterations_spg = softmax_policy_gradient(env, gamma=gamma)\n",
    "        Q_spg = {s: softmax(theta_spg[s]) for s in range(env.observation_space.n)}\n",
    "        reward_spg = evaluate_policy(env, Q_spg)\n",
    "\n",
    "        theta_npg, iterations_npg = natural_policy_gradient(env, gamma=gamma)\n",
    "        Q_npg = {s: softmax(theta_npg[s]) for s in range(env.observation_space.n)}\n",
    "        reward_npg = evaluate_policy(env, Q_npg)\n",
    "\n",
    "        D_inf = 1\n",
    "        S = env.observation_space.n\n",
    "        A = env.action_space.n\n",
    "        print(D_inf, S, A, gamma, epsilon)\n",
    "\n",
    "        theoretical_pga = theoretical_iterations_pga(D_inf, S, A, gamma, epsilon)\n",
    "        theoretical_spg = theoretical_iterations_spg(D_inf, S, A, gamma, epsilon)\n",
    "        theoretical_npg = theoretical_iterations_npg(gamma, epsilon)\n",
    "\n",
    "        results.append({\n",
    "            'gamma': gamma,\n",
    "            'epsilon': epsilon,\n",
    "            'Projected Gradient Ascent': {'empirical': iterations_pga, 'theoretical': theoretical_pga, 'reward': reward_pga},\n",
    "            'Softmax Policy Gradient': {'empirical': iterations_spg, 'theoretical': theoretical_spg, 'reward': reward_spg},\n",
    "            'Natural Policy Gradient': {'empirical': iterations_npg, 'theoretical': theoretical_npg, 'reward': reward_npg},\n",
    "        })\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Gamma: {result['gamma']}, Epsilon: {result['epsilon']}\")\n",
    "    for method in ['Projected Gradient Ascent', 'Softmax Policy Gradient', 'Natural Policy Gradient']:\n",
    "        print(f\"{method}: Empirical = {result[method]['empirical']}, Theoretical = {result[method]['theoretical']}, Reward = {result[method]['reward']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_results(results):\n",
    "    for method in ['Projected Gradient Ascent', 'Softmax Policy Gradient', 'Natural Policy Gradient']:\n",
    "        empirical_values = [result[method]['empirical'] for result in results]\n",
    "        theoretical_values = [result[method]['theoretical'] for result in results]\n",
    "        gamma_epsilon_pairs = [(result['gamma'], result['epsilon']) for result in results]\n",
    "\n",
    "        x = np.arange(len(gamma_epsilon_pairs))\n",
    "        width = 0.35\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(x - width/2, empirical_values, width, label='Empirical')\n",
    "        rects2 = ax.bar(x + width/2, theoretical_values, width, label='Theoretical')\n",
    "\n",
    "        ax.set_xlabel('(Gamma, Epsilon)')\n",
    "        ax.set_ylabel('Iterations')\n",
    "        ax.set_title(f'Empirical vs Theoretical Iterations for {method}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(gamma_epsilon_pairs)\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_star: 0.96059601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yanis/miniforge3/envs/cs234_hw3/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409035ab996c4f55ad1a1b322292fe57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 3, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 4, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 5, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 6, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 7, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 8, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 9, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 10, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 11, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 12, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 13, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 14, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 15, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 16, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 17, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 18, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 19, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 20, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 21, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 22, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 23, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 24, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 25, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 26, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 27, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 28, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 29, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 30, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 31, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 32, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 33, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 34, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 35, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 36, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 37, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 38, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 39, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 40, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 41, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 42, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 43, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 44, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 45, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 46, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 47, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 48, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 49, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 50, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 51, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 52, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 53, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 54, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 55, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 56, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 57, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 58, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 59, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 60, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 61, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 62, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 63, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 64, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 65, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 66, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 67, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 68, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 69, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 70, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 71, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 72, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 73, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 74, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 75, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 76, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 77, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 78, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 79, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 80, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 81, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 82, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 83, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 84, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 85, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 86, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 87, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 88, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 89, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 90, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 91, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 92, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 93, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 94, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 95, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 96, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 97, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 98, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 99, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 100, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 101, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 102, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 103, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 104, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 105, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 106, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 107, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 108, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 109, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 110, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 111, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 112, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 113, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 114, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 115, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 116, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 117, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 118, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 119, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 120, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 121, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 122, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 123, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 124, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 125, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 126, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 127, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 128, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 129, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 130, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 131, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 132, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 133, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 134, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 135, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 136, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 137, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 138, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 139, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 140, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 141, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 142, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 143, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 144, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 145, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 146, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 147, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 148, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 149, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 150, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 151, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 152, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 153, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 154, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 155, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 156, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 157, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 158, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 159, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 160, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 161, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 162, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 163, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 164, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 165, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 166, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 167, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 168, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 169, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 170, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 171, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 172, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 173, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 174, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 175, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 176, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 177, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 178, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 179, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 180, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 181, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 182, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 183, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 184, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 185, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 186, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 187, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 188, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 189, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 190, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 191, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 192, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 193, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 194, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 195, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 196, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 197, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 198, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 199, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 200, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 201, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 202, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 203, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 204, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 205, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 206, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 207, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 208, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 209, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 210, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 211, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 212, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 213, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 214, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 215, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 216, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 217, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 218, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 219, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 220, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 221, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 222, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 223, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 224, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 225, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 226, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 227, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 228, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 229, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 230, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 231, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 232, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 233, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 234, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 235, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 236, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 237, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 238, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 239, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 240, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 241, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 242, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 243, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 244, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 245, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 246, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 247, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 248, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 249, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 250, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 251, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 252, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 253, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 254, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 255, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 256, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 257, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 258, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 259, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 260, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 261, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 262, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 263, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 264, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 265, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 266, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 267, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 268, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 269, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 270, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 271, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 272, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 273, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 274, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 275, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 276, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 277, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 278, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 279, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 280, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 281, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 282, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 283, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 284, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 285, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 286, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 287, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 288, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 289, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 290, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 291, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 292, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 293, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 294, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 295, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 296, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 297, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 298, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 299, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 300, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 301, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 302, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 303, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 304, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 305, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 306, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 307, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 308, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 309, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 310, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 311, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 312, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 313, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 314, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 315, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 316, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 317, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 318, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 319, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 320, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 321, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 322, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 323, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 324, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 325, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 326, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 327, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 328, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 329, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 330, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 331, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 332, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 333, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 334, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 335, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 336, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 337, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 338, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 339, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 340, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 341, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 342, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 343, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 344, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 345, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 346, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 347, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 348, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 349, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 350, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 351, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 352, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 353, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 354, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 355, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 356, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 357, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 358, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 359, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 360, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 361, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 362, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 363, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 364, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 365, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 366, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 367, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 368, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 369, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 370, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 371, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 372, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 373, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 374, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 375, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 376, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 377, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 378, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 379, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 380, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 381, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 382, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 383, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 384, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 385, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 386, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 387, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 388, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 389, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 390, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 391, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 392, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 393, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 394, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 395, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 396, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 397, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 398, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 399, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 400, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 401, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 402, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 403, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 404, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 405, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 406, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 407, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 408, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 409, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 410, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 411, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 412, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 413, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 414, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 415, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 416, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 417, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 418, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 419, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 420, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 421, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 422, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 423, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 424, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 425, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 426, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 427, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 428, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 429, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 430, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 431, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 432, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 433, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 434, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 435, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 436, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 437, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 438, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 439, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 440, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 441, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 442, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 443, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 444, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 445, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 446, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 447, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 448, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 449, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 450, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 451, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 452, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 453, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 454, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 455, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 456, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 457, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 458, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 459, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 460, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 461, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 462, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 463, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 464, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 465, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 466, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 467, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 468, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 469, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 470, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 471, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 472, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 473, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 474, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 475, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 476, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 477, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 478, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 479, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 480, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 481, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 482, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 483, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 484, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 485, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 486, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 487, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 488, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 489, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 490, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 491, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 492, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 493, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 494, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 495, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 496, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 497, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 498, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 499, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 500, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 501, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 502, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 503, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 504, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 505, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 506, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 507, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 508, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 509, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 510, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 511, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 512, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 513, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 514, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 515, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 516, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 517, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 518, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 519, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 520, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 521, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 522, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 523, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 524, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 525, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 526, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 527, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 528, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 529, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 530, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 531, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 532, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 533, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 534, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 535, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 536, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 537, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 538, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 539, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 540, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 541, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 542, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 543, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 544, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 545, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 546, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 547, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 548, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 549, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 550, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 551, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 552, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 553, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 554, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 555, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 556, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 557, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 558, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 559, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 560, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 561, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 562, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 563, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 564, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 565, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 566, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 567, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 568, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 569, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 570, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 571, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 572, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 573, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 574, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 575, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 576, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 577, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 578, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 579, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 580, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 581, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 582, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 583, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 584, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 585, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 586, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 587, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 588, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 589, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 590, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 591, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 592, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 593, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 594, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 595, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 596, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 597, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 598, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 599, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 600, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 601, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 602, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 603, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 604, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 605, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 606, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 607, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 608, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 609, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 610, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 611, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 612, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 613, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 614, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 615, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 616, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 617, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 618, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 619, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 620, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 621, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 622, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 623, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 624, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 625, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 626, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 627, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 628, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 629, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 630, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 631, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 632, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 633, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 634, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 635, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 636, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 637, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 638, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 639, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 640, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 641, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 642, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 643, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 644, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 645, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 646, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 647, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 648, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 649, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 650, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 651, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 652, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 653, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 654, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 655, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 656, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 657, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 658, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 659, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 660, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 661, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 662, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 663, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 664, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 665, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 666, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 667, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 668, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 669, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 670, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 671, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 672, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 673, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 674, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 675, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 676, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 677, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 678, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 679, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 680, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 681, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 682, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 683, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 684, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 685, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 686, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 687, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 688, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 689, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 690, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 691, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 692, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 693, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 694, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 695, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 696, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 697, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 698, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 699, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 700, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 701, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 702, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 703, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 704, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 705, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 706, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 707, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 708, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 709, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 710, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 711, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 712, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 713, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 714, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 715, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 716, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 717, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 718, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 719, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 720, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 721, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 722, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 723, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 724, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 725, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 726, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 727, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 728, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 729, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 730, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 731, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 732, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 733, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 734, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 735, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 736, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 737, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 738, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 739, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 740, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 741, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 742, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 743, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 744, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 745, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 746, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 747, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 748, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 749, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 750, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 751, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 752, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 753, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 754, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 755, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 756, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 757, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 758, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 759, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 760, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 761, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 762, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 763, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 764, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 765, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 766, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 767, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 768, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 769, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 770, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 771, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 772, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 773, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 774, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 775, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 776, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 777, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 778, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 779, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 780, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 781, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 782, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 783, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 784, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 785, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 786, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 787, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 788, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 789, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 790, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 791, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 792, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 793, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 794, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 795, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 796, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 797, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 798, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 799, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 800, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 801, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 802, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 803, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 804, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 805, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 806, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 807, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 808, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 809, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 810, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 811, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 812, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 813, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 814, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 815, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 816, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 817, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 818, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 819, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 820, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 821, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 822, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 823, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 824, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 825, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 826, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 827, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 828, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 829, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 830, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 831, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 832, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 833, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 834, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 835, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 836, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 837, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 838, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 839, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 840, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 841, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 842, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 843, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 844, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 845, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 846, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 847, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 848, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 849, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 850, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 851, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 852, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 853, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 854, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 855, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 856, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 857, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 858, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 859, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 860, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 861, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 862, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 863, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 864, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 865, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 866, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 867, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 868, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 869, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 870, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 871, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 872, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 873, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 874, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 875, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 876, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 877, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 878, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 879, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 880, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 881, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 882, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 883, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 884, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 885, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 886, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 887, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 888, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 889, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 890, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 891, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 892, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 893, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 894, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 895, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 896, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 897, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 898, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 899, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 900, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 901, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 902, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 903, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 904, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 905, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 906, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 907, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 908, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 909, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 910, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 911, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 912, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 913, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 914, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 915, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 916, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 917, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 918, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 919, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 920, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 921, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 922, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 923, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 924, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 925, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 926, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 927, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 928, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 929, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 930, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 931, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 932, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 933, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 934, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 935, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 936, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 937, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 938, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 939, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 940, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 941, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 942, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 943, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 944, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 945, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 946, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 947, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 948, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 949, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 950, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 951, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 952, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 953, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 954, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 955, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 956, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 957, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 958, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 959, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 960, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 961, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 962, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 963, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 964, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 965, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 966, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 967, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 968, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 969, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 970, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 971, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 972, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 973, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 974, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 975, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 976, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 977, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 978, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 979, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 980, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 981, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 982, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 983, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 984, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 985, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 986, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 987, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 988, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 989, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 990, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 991, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 992, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 993, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 994, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 995, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 996, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 997, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 998, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 999, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1000, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1001, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1002, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1003, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1004, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1005, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1006, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1007, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1008, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1009, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1010, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1011, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1012, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1013, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1014, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1015, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1016, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1017, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1018, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1019, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1020, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1021, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1022, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1023, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1024, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1025, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1026, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1027, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1028, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1029, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1030, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1031, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1032, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1033, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1034, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1035, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1036, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1037, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1038, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1039, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1040, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1041, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1042, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1043, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1044, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1045, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1046, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1047, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1048, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1049, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1050, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1051, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1052, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1053, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1054, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1055, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1056, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1057, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1058, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1059, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1060, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1061, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1062, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1063, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1064, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1065, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1066, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1067, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1068, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1069, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1070, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1071, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1072, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1073, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1074, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1075, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1076, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1077, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1078, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1079, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1080, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1081, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1082, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1083, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1084, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1085, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1086, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1087, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1088, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1089, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1090, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1091, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1092, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1093, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1094, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1095, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1096, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1097, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1098, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1099, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1100, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1101, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1102, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1103, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1104, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1105, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1106, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1107, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1108, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1109, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1110, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1111, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1112, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1113, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1114, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1115, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1116, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1117, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1118, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1119, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1120, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1121, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1122, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1123, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1124, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1125, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1126, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1127, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1128, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1129, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1130, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1131, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1132, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1133, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1134, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1135, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1136, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1137, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1138, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1139, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1140, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1141, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1142, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1143, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1144, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1145, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1146, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1147, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1148, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1149, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1150, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1151, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1152, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1153, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1154, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1155, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1156, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1157, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1158, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1159, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1160, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1161, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1162, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1163, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1164, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1165, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1166, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1167, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1168, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1169, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1170, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1171, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1172, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1173, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1174, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1175, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1176, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1177, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1178, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1179, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1180, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1181, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1182, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1183, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1184, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1185, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1186, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1187, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1188, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1189, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1190, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1191, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1192, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1193, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1194, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1195, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1196, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1197, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1198, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1199, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1200, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1201, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1202, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1203, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1204, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1205, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1206, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1207, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1208, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1209, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1210, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1211, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1212, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1213, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1214, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1215, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1216, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1217, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1218, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1219, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1220, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1221, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1222, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1223, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1224, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1225, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1226, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1227, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1228, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1229, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1230, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1231, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1232, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1233, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1234, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1235, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1236, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1237, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1238, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1239, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1240, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1241, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1242, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1243, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1244, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1245, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1246, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1247, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1248, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1249, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1250, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1251, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1252, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1253, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1254, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1255, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1256, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1257, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1258, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1259, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1260, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1261, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1262, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1263, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1264, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1265, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1266, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1267, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1268, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1269, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1270, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1271, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1272, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1273, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1274, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1275, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1276, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1277, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1278, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1279, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1280, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1281, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1282, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1283, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1284, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1285, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1286, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1287, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1288, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1289, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1290, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1291, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1292, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1293, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1294, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1295, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1296, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1297, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1298, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1299, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1300, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1301, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1302, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1303, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1304, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1305, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1306, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1307, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1308, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1309, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1310, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1311, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1312, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1313, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1314, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1315, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1316, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1317, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1318, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1319, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1320, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1321, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1322, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1323, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1324, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1325, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1326, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1327, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1328, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1329, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1330, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1331, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1332, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1333, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1334, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1335, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1336, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1337, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1338, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1339, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1340, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1341, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1342, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1343, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1344, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1345, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1346, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1347, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1348, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1349, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1350, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1351, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1352, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1353, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1354, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1355, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1356, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1357, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1358, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1359, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1360, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1361, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1362, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1363, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1364, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1365, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1366, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1367, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1368, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1369, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1370, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1371, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1372, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1373, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1374, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1375, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1376, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1377, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1378, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1379, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1380, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1381, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1382, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1383, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1384, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1385, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1386, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1387, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1388, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1389, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1390, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1391, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1392, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1393, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1394, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1395, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1396, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1397, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1398, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1399, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1400, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1401, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1402, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1403, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1404, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1405, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1406, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1407, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1408, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1409, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1410, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1411, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1412, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1413, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1414, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1415, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1416, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1417, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1418, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1419, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1420, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1421, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1422, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1423, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1424, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1425, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1426, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1427, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1428, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1429, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1430, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1431, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1432, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1433, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1434, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1435, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1436, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1437, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1438, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1439, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1440, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1441, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1442, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1443, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1444, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1445, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1446, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1447, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1448, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1449, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1450, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1451, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1452, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1453, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1454, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1455, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1456, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1457, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1458, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1459, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1460, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1461, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1462, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1463, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1464, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1465, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1466, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1467, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1468, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1469, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1470, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1471, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1472, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1473, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1474, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1475, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1476, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1477, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1478, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1479, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1480, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1481, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1482, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1483, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1484, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1485, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1486, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1487, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1488, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1489, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1490, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1491, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1492, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1493, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1494, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1495, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1496, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1497, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1498, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1499, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1500, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1501, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1502, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1503, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1504, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1505, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1506, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1507, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1508, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1509, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1510, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1511, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1512, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1513, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1514, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1515, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1516, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1517, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1518, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1519, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1520, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1521, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1522, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1523, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1524, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1525, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1526, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1527, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1528, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1529, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1530, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1531, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1532, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1533, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1534, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1535, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1536, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1537, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1538, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1539, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1540, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1541, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1542, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1543, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1544, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1545, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1546, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1547, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1548, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1549, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1550, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1551, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1552, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1553, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1554, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1555, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1556, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1557, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1558, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1559, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1560, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1561, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1562, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1563, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1564, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1565, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1566, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1567, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1568, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1569, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1570, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1571, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1572, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1573, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1574, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1575, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1576, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1577, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1578, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1579, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1580, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1581, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1582, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1583, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1584, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1585, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1586, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1587, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1588, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1589, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1590, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1591, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1592, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1593, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1594, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1595, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1596, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1597, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1598, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1599, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1600, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1601, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1602, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1603, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1604, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1605, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1606, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1607, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1608, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1609, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1610, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1611, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1612, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1613, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1614, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1615, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1616, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1617, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1618, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1619, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1620, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1621, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1622, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1623, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1624, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1625, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1626, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1627, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1628, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1629, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1630, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1631, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1632, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1633, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1634, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1635, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1636, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1637, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1638, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1639, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1640, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1641, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1642, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1643, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1644, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1645, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1646, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1647, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1648, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1649, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1650, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1651, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1652, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1653, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1654, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1655, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1656, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1657, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1658, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1659, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1660, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1661, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1662, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1663, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1664, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1665, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1666, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1667, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1668, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1669, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1670, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1671, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1672, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1673, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1674, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1675, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1676, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1677, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1678, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1679, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1680, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1681, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1682, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1683, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1684, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1685, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1686, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1687, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1688, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1689, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1690, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1691, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1692, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1693, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1694, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1695, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1696, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1697, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1698, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1699, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1700, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1701, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1702, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1703, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1704, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1705, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1706, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1707, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1708, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1709, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1710, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1711, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1712, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1713, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1714, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1715, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1716, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1717, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1718, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1719, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1720, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1721, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1722, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1723, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1724, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1725, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1726, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1727, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1728, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1729, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1730, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1731, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1732, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1733, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1734, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1735, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1736, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1737, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1738, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1739, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1740, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1741, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1742, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1743, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1744, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1745, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1746, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1747, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1748, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1749, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1750, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1751, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1752, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1753, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1754, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1755, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1756, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1757, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1758, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1759, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1760, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1761, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1762, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1763, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1764, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1765, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1766, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1767, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1768, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1769, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1770, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1771, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1772, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1773, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1774, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1775, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1776, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1777, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1778, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1779, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1780, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1781, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1782, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1783, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1784, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1785, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1786, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1787, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1788, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1789, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1790, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1791, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1792, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1793, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1794, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1795, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1796, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1797, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1798, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1799, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1800, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1801, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1802, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1803, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1804, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1805, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1806, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1807, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1808, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1809, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1810, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1811, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1812, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1813, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1814, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1815, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1816, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1817, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1818, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1819, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1820, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1821, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1822, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1823, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1824, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1825, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1826, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1827, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1828, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1829, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1830, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1831, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1832, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1833, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1834, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1835, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1836, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1837, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1838, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1839, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1840, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1841, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1842, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1843, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1844, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1845, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1846, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1847, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1848, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1849, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1850, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1851, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1852, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1853, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1854, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1855, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1856, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1857, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1858, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1859, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1860, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1861, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1862, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1863, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1864, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1865, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1866, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1867, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1868, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1869, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1870, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1871, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1872, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1873, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1874, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1875, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1876, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1877, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1878, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1879, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1880, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1881, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1882, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1883, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1884, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1885, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1886, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1887, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1888, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1889, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1890, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1891, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1892, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1893, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1894, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1895, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1896, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1897, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1898, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1899, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1900, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1901, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1902, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1903, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1904, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1905, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1906, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1907, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1908, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1909, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1910, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1911, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1912, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1913, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1914, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1915, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1916, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1917, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1918, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1919, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1920, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1921, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1922, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1923, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1924, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1925, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1926, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1927, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1928, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1929, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1930, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1931, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1932, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1933, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1934, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1935, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1936, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1937, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1938, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1939, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1940, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1941, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1942, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1943, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1944, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1945, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1946, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1947, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1948, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1949, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1950, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1951, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1952, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1953, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1954, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1955, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1956, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1957, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1958, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1959, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1960, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1961, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1962, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1963, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1964, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1965, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1966, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1967, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1968, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1969, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1970, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1971, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1972, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1973, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1974, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1975, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1976, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1977, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1978, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1979, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1980, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1981, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1982, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1983, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1984, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1985, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1986, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1987, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1988, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1989, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1990, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1991, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1992, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1993, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1994, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1995, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1996, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1997, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1998, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 1999, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2000, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2001, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2002, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2003, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2004, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2005, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2006, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2007, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2008, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2009, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2010, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2011, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2012, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2013, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2014, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2015, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2016, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2017, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2018, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2019, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2020, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2021, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2022, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2023, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2024, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2025, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2026, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2027, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2028, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2029, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2030, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2031, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2032, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2033, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2034, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2035, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2036, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2037, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2038, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2039, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2040, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2041, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2042, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2043, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2044, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2045, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2046, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2047, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2048, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2049, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2050, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2051, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2052, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2053, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2054, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2055, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2056, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2057, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2058, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2059, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2060, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2061, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2062, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2063, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2064, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2065, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2066, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2067, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2068, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2069, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2070, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2071, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2072, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2073, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2074, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2075, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2076, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2077, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2078, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2079, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2080, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2081, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2082, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2083, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2084, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2085, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2086, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2087, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2088, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2089, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2090, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2091, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2092, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2093, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2094, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2095, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2096, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2097, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2098, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2099, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2100, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2101, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2102, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2103, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2104, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2105, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2106, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2107, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2108, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2109, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2110, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2111, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2112, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2113, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2114, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2115, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2116, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2117, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2118, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2119, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2120, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2121, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2122, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2123, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2124, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2125, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2126, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2127, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2128, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2129, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2130, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2131, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2132, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2133, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2134, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2135, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2136, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2137, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2138, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2139, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2140, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2141, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2142, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2143, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2144, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2145, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2146, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2147, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2148, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2149, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2150, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2151, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2152, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2153, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2154, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2155, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2156, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2157, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2158, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2159, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2160, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2161, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2162, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2163, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2164, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2165, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2166, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2167, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2168, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2169, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2170, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2171, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2172, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2173, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2174, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2175, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2176, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2177, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2178, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2179, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2180, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2181, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2182, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2183, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2184, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2185, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2186, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2187, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2188, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2189, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2190, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2191, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2192, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2193, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2194, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2195, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2196, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2197, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2198, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2199, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2200, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2201, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2202, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2203, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2204, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2205, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2206, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2207, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2208, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2209, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2210, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2211, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2212, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2213, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2214, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2215, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2216, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2217, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2218, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2219, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2220, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2221, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2222, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2223, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2224, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2225, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2226, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2227, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2228, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2229, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2230, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2231, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2232, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2233, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2234, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2235, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2236, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2237, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2238, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2239, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2240, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2241, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2242, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2243, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2244, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2245, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2246, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2247, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2248, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2249, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2250, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2251, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2252, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2253, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2254, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2255, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2256, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2257, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2258, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2259, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2260, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2261, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2262, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2263, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2264, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2265, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2266, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2267, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2268, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2269, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2270, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2271, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2272, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2273, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2274, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2275, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2276, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2277, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2278, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2279, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2280, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2281, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2282, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2283, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2284, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2285, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2286, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2287, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2288, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2289, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2290, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2291, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2292, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2293, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2294, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2295, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2296, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2297, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2298, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2299, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2300, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2301, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2302, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2303, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2304, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2305, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2306, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2307, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2308, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2309, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2310, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2311, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2312, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2313, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2314, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2315, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2316, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2317, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2318, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2319, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2320, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2321, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2322, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2323, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2324, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2325, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2326, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2327, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2328, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2329, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2330, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2331, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2332, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2333, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2334, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2335, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2336, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2337, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2338, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2339, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2340, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2341, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2342, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2343, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2344, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2345, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2346, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2347, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2348, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2349, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2350, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2351, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2352, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2353, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2354, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2355, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2356, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2357, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2358, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2359, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2360, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2361, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2362, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2363, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2364, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2365, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2366, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2367, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2368, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2369, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2370, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2371, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2372, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2373, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2374, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2375, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2376, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2377, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2378, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2379, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2380, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2381, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2382, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2383, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2384, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Iteration 2385, V_pi[s0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m s0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Fixed starting state\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Run Projected Gradient Ascent\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m Q_pga, iterations_pga \u001b[38;5;241m=\u001b[39m \u001b[43mprojected_gradient_ascent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m reward_pga \u001b[38;5;241m=\u001b[39m evaluate_policy(env, Q_pga)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProjected Gradient Ascent: Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_pga\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Iterations = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miterations_pga\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[105], line 67\u001b[0m, in \u001b[0;36mprojected_gradient_ascent\u001b[0;34m(env, gamma, alpha, epsilon, max_iterations, s0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     66\u001b[0m     policy \u001b[38;5;241m=\u001b[39m epsilon_greedy_policy(Q, state, nA)\n\u001b[0;32m---> 67\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     70\u001b[0m     best_next_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[next_state])\n",
      "File \u001b[0;32mmtrand.pyx:941\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/cs234_hw3/lib/python3.8/site-packages/numpy/core/numerictypes.py:356\u001b[0m, in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    Determine if the first argument is a subclass of the second argument.\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(obj2sctype(arg1), obj2sctype(arg2))\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;129m@set_module\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missubdtype\u001b[39m(arg1, arg2):\n\u001b[1;32m    358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    Returns True if first argument is a typecode lower/equal in type hierarchy.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issubclass_(arg1, generic):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def one_hot(n, i):\n",
    "    vec = np.zeros(n)\n",
    "    vec[i] = 1\n",
    "    return vec\n",
    "\n",
    "def epsilon_greedy_policy(Q, state, nA, epsilon=0.1):\n",
    "    policy = np.ones(nA) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    policy[best_action] += (1.0 - epsilon)\n",
    "    return policy\n",
    "\n",
    "def value_iteration(env, gamma=0.99, epsilon=0.01, max_iterations=10000):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        prev_V = np.copy(V)\n",
    "        for state in range(nS):\n",
    "            Q_values = np.zeros(nA)\n",
    "            for action in range(nA):\n",
    "                q_value = 0\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    q_value += prob * (reward + gamma * prev_V[next_state] * (not done))\n",
    "                Q_values[action] = q_value\n",
    "            V[state] = np.max(Q_values)\n",
    "        if np.max(np.abs(prev_V - V)) < epsilon:\n",
    "            break\n",
    "\n",
    "    return V\n",
    "\n",
    "def evaluate_policy(env, Q, num_episodes=100, max_steps_per_episode=100):\n",
    "    total_rewards = 0\n",
    "    for _ in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done and steps < max_steps_per_episode:\n",
    "            action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_rewards += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "    return total_rewards / num_episodes\n",
    "\n",
    "\n",
    "\n",
    "def projected_gradient_ascent(env, gamma=0.99, alpha=0.1, epsilon=0.01, max_iterations=10000, s0=0):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    iteration_count = 0\n",
    "    V_star = value_iteration(env, gamma, epsilon, max_iterations)[s0]\n",
    "    print(f\"V_star: {V_star}\")\n",
    "\n",
    "    for _ in tqdm(range(max_iterations)):\n",
    "        iteration_count += 1\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            policy = epsilon_greedy_policy(Q, state, nA)\n",
    "            action = np.random.choice(np.arange(nA), p=policy)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_error\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # Check the stopping criterion for the fixed state s0\n",
    "        V_pi = np.max(Q, axis=1)\n",
    "        print(f\"Iteration {iteration_count}, V_pi[s0]: {V_pi}\")\n",
    "        if V_star - V_pi[s0] <= epsilon:\n",
    "            break\n",
    "\n",
    "    return Q, iteration_count\n",
    "\n",
    "# Parameters\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "gamma = 0.99\n",
    "alpha = 1\n",
    "epsilon = 0.5\n",
    "s0 = 1  # Fixed starting state\n",
    "\n",
    "# Run Projected Gradient Ascent\n",
    "Q_pga, iterations_pga = projected_gradient_ascent(env, gamma, alpha, epsilon, s0=s0)\n",
    "reward_pga = evaluate_policy(env, Q_pga)\n",
    "\n",
    "print(f\"Projected Gradient Ascent: Reward = {reward_pga}, Iterations = {iterations_pga}\")\n",
    "\n",
    "# Theoretical Iterations\n",
    "D_inf = 0.1\n",
    "S = env.observation_space.n\n",
    "A = env.action_space.n\n",
    "\n",
    "def theoretical_iterations_pga(D_inf, S, A, gamma, epsilon):\n",
    "    return (D_inf**2 * S * A) / ((1 - gamma)**6 * epsilon**2)\n",
    "\n",
    "theoretical_pga = theoretical_iterations_pga(D_inf, S, A, gamma, epsilon)\n",
    "print(f\"Theoretical Projected Gradient Ascent Iterations: {theoretical_pga}\")\n",
    "\n",
    "# Plotting\n",
    "labels = ['Empirical', 'Theoretical']\n",
    "values = [iterations_pga, theoretical_pga]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Iteration Type')\n",
    "plt.ylabel('Number of Iterations')\n",
    "plt.title('Empirical vs Theoretical Iterations for Projected Gradient Ascent')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs234_hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
